{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#IV: Making the Graph"
      ],
      "metadata": {
        "id": "LZOvgeyUkygS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Library Imports"
      ],
      "metadata": {
        "id": "H0jBAvBKk10-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM2THA6-kIKj",
        "outputId": "c2fb5b42-961f-46a6-e6c6-8729fa5fbae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/316.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.4/316.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ipython-sql sqlalchemy prettytable==3.8.0 networkx lightfm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import json\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "O2z8Ikn1k7h5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload files (udf1 and rdf1)"
      ],
      "metadata": {
        "id": "qs-U7OE1A5pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Upload udf1.csv and rdf1.csv\")\n",
        "uploaded = files.upload()\n",
        "udf1 = pd.read_csv('udf1.csv')\n",
        "rdf1 = pd.read_csv('rdf1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "yc9VVo1dA47E",
        "outputId": "bd9737d9-3cf6-43e2-d88c-c9a1c77b1198"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload udf1.csv and rdf1.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8710fb68-0009-443f-b7ff-1ee8e8d6f555\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8710fb68-0009-443f-b7ff-1ee8e8d6f555\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving udf1.csv to udf1.csv\n",
            "Saving rdf1.csv to rdf1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "HjcgY69mwUoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean udf1: Handle missing values\n",
        "udf1 = udf1.fillna({\n",
        "    'age': udf1['age'].median(),\n",
        "    'gender': 'unknown',\n",
        "    'city': 'unknown',\n",
        "    'most_viewed_sport': 'unknown',\n",
        "    'most_viewed_entity': 'unknown',\n",
        "    'TSS': 0,\n",
        "    'SLI': 0\n",
        "})\n",
        "\n",
        "# Normalize TSS and SLI\n",
        "udf1['TSS_norm'] = (udf1['TSS'] - udf1['TSS'].min()) / (udf1['TSS'].max() - udf1['TSS'].min() + 1e-6)\n",
        "udf1['SLI_norm'] = (udf1['SLI'] - udf1['SLI'].min()) / (udf1['SLI'].max() - udf1['SLI'].min() + 1e-6)"
      ],
      "metadata": {
        "id": "yrs_WODMvUmj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse dictionary columns\n",
        "udf1['sports_dict'] = udf1['sports_with_visit_count_dict'].apply(\n",
        "    lambda x: json.loads(x.replace(\"'\", '\"')) if isinstance(x, str) else x\n",
        ")\n",
        "udf1['entity_dict'] = udf1['entity_with_visit_count_dict'].apply(\n",
        "    lambda x: json.loads(x.replace(\"'\", '\"')) if isinstance(x, str) else x\n",
        ")"
      ],
      "metadata": {
        "id": "7dGTBWBoyuBn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean rdf1: Remove duplicates and filter related posts\n",
        "rdf1 = rdf1[rdf1['source'].str.startswith('related_post')].drop_duplicates(\n",
        "    subset=['ppid', 'slug', 'timestamp']\n",
        ")\n",
        "\n",
        "# Handle missing values in rdf1\n",
        "rdf1 = rdf1.fillna({'sport_from_slug': 'unknown', 'persons': '', 'orgs': ''})\n",
        "\n",
        "# Debug: Check data shapes and duplicates\n",
        "print(f\"udf1 shape: {udf1.shape}\")\n",
        "print(f\"rdf1 shape after deduplication: {rdf1.shape}\")\n",
        "print(f\"Duplicate rows in rdf1: {rdf1.duplicated(subset=['ppid', 'slug', 'timestamp']).sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kktzhv8jwZEr",
        "outputId": "4b4ab2cc-cda3-4bca-90f1-2d7db16ec8bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "udf1 shape: (10000, 38)\n",
            "rdf1 shape after deduplication: (2184, 12)\n",
            "Duplicate rows in rdf1: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph Building"
      ],
      "metadata": {
        "id": "_wsU6Phgwfpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add user nodes\n",
        "for _, row in udf1.iterrows():\n",
        "    G.add_node(row['ppid'], type='user', age=row['age'], gender=row['gender'],\n",
        "               visit_count=row['visit_count'], timeonpage=row['timeonpage'],\n",
        "               most_viewed_sport=row['most_viewed_sport'],\n",
        "               most_viewed_entity=row['most_viewed_entity'],\n",
        "               TSS=row['TSS'], SLI=row['SLI'])"
      ],
      "metadata": {
        "id": "S-_66-5Twmgu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding article nodes"
      ],
      "metadata": {
        "id": "JQxv8jAixIdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add article nodes\n",
        "for _, row in rdf1.iterrows():\n",
        "    G.add_node(row['slug'], type='article', sport=row['sport_from_slug'],\n",
        "               persons=row['persons'], orgs=row['orgs'])\n",
        "\n",
        "# Add sport nodes\n",
        "sports = set()\n",
        "for d in udf1['sports_dict']:\n",
        "    sports.update(d.keys())\n",
        "sports.update(rdf1['sport_from_slug'].dropna())\n",
        "for sport in sports:\n",
        "    G.add_node(sport, type='sport')\n",
        "\n",
        "# Add entity nodes\n",
        "entities = set()\n",
        "for d in udf1['entity_dict']:\n",
        "    entities.update(d.keys())\n",
        "for p in rdf1['persons'].dropna().str.split(',').explode().str.strip():\n",
        "    entities.add(p)\n",
        "for entity in entities:\n",
        "    G.add_node(entity, type='entity')"
      ],
      "metadata": {
        "id": "SOzWi4sUwnJ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding edges"
      ],
      "metadata": {
        "id": "nKGEj2XYxLo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add edges: User -> Article (CLICKS)\n",
        "click_counts = rdf1.groupby(['ppid', 'slug']).size().reset_index(name='click_count')\n",
        "for _, row in click_counts.iterrows():\n",
        "    if row['ppid'] in G.nodes and row['slug'] in G.nodes:\n",
        "        user_tss = udf1[udf1['ppid'] == row['ppid']]['TSS'].iloc[0]\n",
        "        user_sli = udf1[udf1['ppid'] == row['ppid']]['SLI'].iloc[0]\n",
        "        weight = row['click_count'] * (user_tss + user_sli) / 2\n",
        "        G.add_edge(row['ppid'], row['slug'], type='clicks', weight=max(weight, 1))\n",
        "\n",
        "# Add edges: User -> Sport (INTERESTED_IN)\n",
        "for _, row in udf1.iterrows():\n",
        "    for sport, count in row['sports_dict'].items():\n",
        "        if sport in G.nodes:\n",
        "            G.add_edge(row['ppid'], sport, type='interested_in', weight=count)"
      ],
      "metadata": {
        "id": "Kjghcr1Ywm9J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add edges: User -> Entity (INTERESTED_IN)\n",
        "for _, row in udf1.iterrows():\n",
        "    for entity, count in row['entity_dict'].items():\n",
        "        if entity in G.nodes:\n",
        "            G.add_edge(row['ppid'], entity, type='interested_in', weight=count)\n",
        "\n",
        "# Add edges: Article -> Sport (BELONGS_TO)\n",
        "for _, row in rdf1.iterrows():\n",
        "    if row['sport_from_slug'] in G.nodes:\n",
        "        G.add_edge(row['slug'], row['sport_from_slug'], type='belongs_to', weight=1)\n",
        "\n",
        "# Add edges: Article -> Entity (FEATURES)\n",
        "for _, row in rdf1.iterrows():\n",
        "    if pd.notna(row['persons']):\n",
        "        for person in row['persons'].split(','):\n",
        "            person = person.strip()\n",
        "            if person in G.nodes:\n",
        "                G.add_edge(row['slug'], person, type='features', weight=1)"
      ],
      "metadata": {
        "id": "i3pXfo3pwmzw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mini Debugging"
      ],
      "metadata": {
        "id": "TeLO38ZexQgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Check graph size and connectivity\n",
        "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Number of edges: {G.number_of_edges()}\")\n",
        "user_id = 'vtpks1740535964462a17db399a96d'\n",
        "if user_id in G.nodes:\n",
        "    print(f\"Edges from user {user_id}: {list(G.out_edges(user_id, data=True))}\")\n",
        "else:\n",
        "    print(f\"User {user_id} not in graph\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skoSD6e-w_J-",
        "outputId": "af271710-a69c-45a2-9aa8-ad4ffbc377af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 10835\n",
            "Number of edges: 52807\n",
            "Edges from user vtpks1740535964462a17db399a96d: [('vtpks1740535964462a17db399a96d', 'nfl-active-news-after-ditching-mike-tomlins-steelers-justin-fields-celebrates-usd-forty-m-contract-in-style', {'type': 'clicks', 'weight': np.float64(1.4444444444444444)}), ('vtpks1740535964462a17db399a96d', 'nfl', {'type': 'interested_in', 'weight': 9}), ('vtpks1740535964462a17db399a96d', 'Others', {'type': 'interested_in', 'weight': 8}), ('vtpks1740535964462a17db399a96d', ' John Harbaugh', {'type': 'interested_in', 'weight': 1})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the dataset"
      ],
      "metadata": {
        "id": "I4uttMwnxS7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save edge list and node attributes\n",
        "nx.write_edgelist(G, 'graph_edgelist.txt')\n",
        "node_data = pd.DataFrame({node: attr for node, attr in G.nodes(data=True)}).T\n",
        "node_data.to_csv('node_attributes.csv')"
      ],
      "metadata": {
        "id": "TidZyPFZxZAl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Recommendation Algorithms"
      ],
      "metadata": {
        "id": "Ycrx-FuIxeq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Graph Based Recommendation Algorithm -> Personalized PageRank Algorithm (PPR)\n",
        "\n",
        "- Personalized Recommendations: PPR ranks articles by propagating importance from a specific user node, leveraging their clicks (```rdf1```) and preferences (```sports_with_visit_count_dict```, ```most_viewed_sport in udf1```) to suggest relevant articles for the Related Posts widget.\n",
        "\n",
        "- Graph-Based Relevance: Utilizes the heterogeneous graph of users, articles, sports, and entities, with weighted edges (e.g., click counts, TSS/SLI-scaled weights) to capture user behavior and article relationships, ensuring robust recommendations.\n",
        "\n",
        "- Efficient and Scalable: With a tuned damping factor (e.g., ```alpha=0.3```) and personalization boosts for user preferences, PPR efficiently handles the 10K-user dataset, avoiding zero scores and supporting real-time recommendations."
      ],
      "metadata": {
        "id": "-VR5S1CRxnyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate recommendations"
      ],
      "metadata": {
        "id": "dVZ5ssKeAAAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run PPR for recommendations\n",
        "user_id = 'vtpks1740535964462a17db399a96d'\n",
        "if user_id in G.nodes:\n",
        "    personalization = {node: 1.0 if node == user_id else 0.0 for node in G.nodes}\n",
        "    ppr_scores = nx.pagerank(G, alpha=0.3, personalization=personalization,\n",
        "                             weight='weight', max_iter=100, tol=1e-6)\n",
        "    article_scores = {node: score for node, score in ppr_scores.items()\n",
        "                      if G.nodes[node]['type'] == 'article'}\n",
        "    top_articles = sorted(article_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "    print(f\"Top 3 Recommended Articles for User {user_id}:\")\n",
        "    for article, score in top_articles:\n",
        "        sport = G.nodes[article]['sport']\n",
        "        print(f\"Article: {article}, Sport: {sport}\")\n",
        "else:\n",
        "    print(f\"User {user_id} not found in graph\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLbx5L9NxzDt",
        "outputId": "ff1e608f-3799-4dd6-dca4-1d89e4096b95"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Recommended Articles for User vtpks1740535964462a17db399a96d:\n",
            "Article: nfl-active-news-after-ditching-mike-tomlins-steelers-justin-fields-celebrates-usd-forty-m-contract-in-style, Sport: NFL\n",
            "Article: nfl-active-news-maxx-crosby-withdraws-antonio-pierce-support-as-tom-bradys-raiders-betray-hc-to-announce-firing, Sport: NFL\n",
            "Article: mlb-baseball-news-blake-snell-reveals-his-one-wholesome-request-from-andrew-friedman-regarding-a-dodgers-legend, Sport: MLB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular ML-Based Recommendation Algorithm -> LightFM"
      ],
      "metadata": {
        "id": "e4KUccaX9c97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute click counts\n",
        "click_counts = rdf1.groupby(['ppid', 'slug']).size().reset_index(name='click_count')"
      ],
      "metadata": {
        "id": "arK8yUJV9bSr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Check data and overlaps\n",
        "print(f\"udf1 shape: {udf1.shape}, Unique ppid: {udf1['ppid'].nunique()}\")\n",
        "print(f\"rdf1 shape: {rdf1.shape}, Unique ppid: {rdf1['ppid'].nunique()}, Unique slug: {rdf1['slug'].nunique()}\")\n",
        "print(f\"click_counts rows: {len(click_counts)}\")\n",
        "common_ppids = set(rdf1['ppid']).intersection(set(udf1['ppid']))\n",
        "print(f\"Common ppid between udf1 and rdf1: {len(common_ppids)}\")\n",
        "if len(common_ppids) == 0:\n",
        "    print(\"Warning: No common ppid. Sample rdf1 ppid:\")\n",
        "    print(rdf1['ppid'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyoST5RH-tj7",
        "outputId": "8534c2d3-7bdd-48c1-b0c0-c0a0c71e914b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "udf1 shape: (10000, 38), Unique ppid: 8813\n",
            "rdf1 shape: (2184, 12), Unique ppid: 740, Unique slug: 1260\n",
            "click_counts rows: 1325\n",
            "Common ppid between udf1 and rdf1: 740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize LightFM Dataset"
      ],
      "metadata": {
        "id": "SB9lr50A_bnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LightFM Dataset\n",
        "dataset = Dataset()\n",
        "\n",
        "# User and item IDs\n",
        "user_ids = udf1['ppid'].unique()\n",
        "item_ids = rdf1['slug'].unique()"
      ],
      "metadata": {
        "id": "e_xFV9Do-QEu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up user features"
      ],
      "metadata": {
        "id": "sVgE12_V_hZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User features: age, TSS_norm, SLI_norm, sports_dict\n",
        "all_sports = set()\n",
        "for d in udf1['sports_dict']:\n",
        "    all_sports.update(d.keys())\n",
        "all_sports = sorted(list(all_sports))\n",
        "user_features = []\n",
        "for _, row in udf1.iterrows():\n",
        "    features = [f\"age:{row['age']}\", f\"TSS_norm:{row['TSS_norm']:.2f}\", f\"SLI_norm:{row['SLI_norm']:.2f}\"]\n",
        "    for sport in all_sports:\n",
        "        count = row['sports_dict'].get(sport, 0)\n",
        "        features.append(f\"sport_{sport}:{np.log1p(count):.2f}\")\n",
        "    user_features.append(features)"
      ],
      "metadata": {
        "id": "RQW3Zhrf-xHu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up item features"
      ],
      "metadata": {
        "id": "3S2Caeyh_oXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Item features: sport_from_slug, has_orgs\n",
        "item_features = []\n",
        "for slug in item_ids:\n",
        "    sport = rdf1[rdf1['slug'] == slug]['sport_from_slug'].iloc[0]\n",
        "    has_orgs = 1 if rdf1[rdf1['slug'] == slug]['orgs'].iloc[0] != '' else 0\n",
        "    item_features.append([f\"sport:{sport}\", f\"has_orgs:{has_orgs}\"])"
      ],
      "metadata": {
        "id": "2SXr3gi8_O0k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting algorithm"
      ],
      "metadata": {
        "id": "Fns_nObj_rji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit dataset\n",
        "dataset.fit(\n",
        "    users=user_ids,\n",
        "    items=item_ids,\n",
        "    user_features=[f for features in user_features for f in features],\n",
        "    item_features=[f for features in item_features for f in features]\n",
        ")"
      ],
      "metadata": {
        "id": "Zrl_Brwr-xCX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connect nodes, edges/weights by building interactions"
      ],
      "metadata": {
        "id": "J6M5gkNX_tbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build interactions\n",
        "interactions, weights = dataset.build_interactions(\n",
        "    [(row['ppid'], row['slug'], row['click_count']) for _, row in click_counts.iterrows()\n",
        "    if row['ppid'] in user_ids and row['slug'] in item_ids]\n",
        ")\n",
        "\n",
        "# Build feature matrices\n",
        "user_feature_matrix = dataset.build_user_features(\n",
        "    [(uid, features) for uid, features in zip(user_ids, user_features)]\n",
        ")\n",
        "item_feature_matrix = dataset.build_item_features(\n",
        "    [(iid, features) for iid, features in zip(item_ids, item_features)]\n",
        ")"
      ],
      "metadata": {
        "id": "16UTXuD3_Lr6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Matrix debugging"
      ],
      "metadata": {
        "id": "0u9WFRIa_z_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Check matrices\n",
        "print(f\"Interactions shape: {interactions.shape}\")\n",
        "print(f\"User features shape: {user_feature_matrix.shape}\")\n",
        "print(f\"Item features shape: {item_feature_matrix.shape}\")\n",
        "print(f\"Non-zero interactions: {interactions.nnz}\")\n",
        "if interactions.nnz == 0:\n",
        "    print(\"Warning: No valid interactions. Sample click_counts:\")\n",
        "    print(click_counts.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MgXR0Ae-w6i",
        "outputId": "f5e4cc32-1e79-41c8-a2e8-af8d1c6b8618"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interactions shape: (8813, 1260)\n",
            "User features shape: (8813, 10101)\n",
            "Item features shape: (1260, 1284)\n",
            "Non-zero interactions: 1325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "GRspj3ud_2lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LightFM model\n",
        "model = LightFM(loss='warp', no_components=30, learning_rate=0.05)\n",
        "model.fit(\n",
        "    interactions,\n",
        "    user_features=user_feature_matrix,\n",
        "    item_features=item_feature_matrix,\n",
        "    epochs=30,\n",
        "    num_threads=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Debug: Check model fit\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cnxsiLk-TJc",
        "outputId": "ee39a2b3-293d-42c7-b092-0e971ab757fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 30/30 [00:00<00:00, 55.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate recommendations"
      ],
      "metadata": {
        "id": "UEXuJt_0_5MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate recommendations\n",
        "user_id = 'vtpks1740535964462a17db399a96d'\n",
        "if user_id in user_ids:\n",
        "    user_idx = dataset.mapping()[0][user_id]\n",
        "    scores = model.predict(user_idx, np.arange(len(item_ids)), user_features=user_feature_matrix, item_features=item_feature_matrix)\n",
        "    top_indices = np.argsort(-scores)[:3]\n",
        "    top_articles = [item_ids[i] for i in top_indices]\n",
        "    top_scores = scores[top_indices]\n",
        "\n",
        "    print(f\"Top 3 Recommended Articles for User {user_id}:\")\n",
        "    for article, score in zip(top_articles, top_scores):\n",
        "        sport = rdf1[rdf1['slug'] == article]['sport_from_slug'].iloc[0] if not rdf1[rdf1['slug'] == article].empty else 'unknown'\n",
        "        print(f\"Article: {article}, Sport: {sport}\")\n",
        "else:\n",
        "    print(f\"User {user_id} not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ARosNiI-ZPe",
        "outputId": "5a7d62fb-e9a8-4514-8553-a47c2393eb04"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Recommended Articles for User vtpks1740535964462a17db399a96d:\n",
            "Article: nba-active-basketball-news-nba-reporter-trish-christakis-s-cbs-outfit-turns-heads-as-miami-heat-support-soars-even-after-disappointing-playoff-exit, Sport: NBA\n",
            "Article: nfl-active-news-kyle-juszczyks-wife-kristin-shares-one-word-reaction-to-husbands-release-as-49ers-decision-breaks-christian-mccaffreys-heart, Sport: NFL\n",
            "Article: ncaa-college-basketball-news-todd-golden-clears-stance-on-selfish-florida-locker-room-as-auburn-exposes-major-gators-problem, Sport: NCAA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THE END (for now)"
      ],
      "metadata": {
        "id": "m2Ixll2gAHCo"
      }
    }
  ]
}